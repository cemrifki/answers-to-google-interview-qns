# ATexPro Light

This document is about helping users install and use ATexPro Light, which stands for Afiniti Text Preprocessor Light version. They can also leverage this package along with ADAIR as will be explained.

ATexPro light helps us create natural language processing (NLP) features, which are as follows:

      1. character
      2. word
      3. word3
      4. SBERT
      5. word2vec

Here, word3 eliminates stopwords differently from the word layer. SBERT features are those generated by multi-lingual sentence transformer (a BERT model). That is, it can create vectors for multiple languages, such as English and French, via the same model. word2vec embeddings are pretrained spaCy word vectors. We can rely on all these features when we have texts, such as short descriptions of products.

## The explanation of code

As mentioned, this code helps us generate NLP vectors that can enrich the existing models. The source code is mainly included in the src folder. The config parameters can be updated in the config folder, which includes the below files:

- data_config.ini
- feat_ext_config.ini

The natural language (e.g. French and English) and other features related to data are included in the data_config.ini. On the other hand, feat_ext_config.ini is relevant to feature layers, such as tf-idf, w2v, and SBERT layers. For example, one can update the dimension size of BERT embeddings, if he or she wants to have embeddings of a lower size. Automatically, this operation is performed by relying on truncated SVD method. Apart from these, the data to be processed is in the data folder and is stated in the experiment.ini file. Relative paths are adapted for this ATexPro light version repo differently from ADAIR. That is, the code can run smoothly for both Linux (e.g. AI server) and Windows (e.g. your local computer). 

## How to run it on the server

We also packaged this ATexPro light into a single wheel file with the models thereof (i.e. 1. Sentence Transformer, 2. spaCy word2vec vectors, 3. NLTK stopwrods). In order to leverage this package, you additionally need only one external .tar.gz, which is for sentence transformer. To use this package, you need to install the required files via the following commands for the ADAIR docker on the AI server (http://10.36.25.93/):

- %pip install --user /home/jovyan/work/nlp_common/bilge.sipal/NLP_team_only/apro_lightv3/sentence-transformers-1.2.0.tar.gz

- %pip install --user /home/jovyan/work/nlp_common/cem.aydin/NLP_team_only/atexpro_light-0.1.0-py3-none-any.whl

These items above involving the use of the sentence-transformer and .whl files can be updated with respect to the path of this file, for example, for the AT&T server. For the docker base-note-book on the AI server, it suffices to install the following package only:

- %pip install --user /home/jovyan/work/nlp_common/cem.aydin/NLP_team_only/atexpro_light-0.1.0-py3-none-any.whl

The CUDA might not work for the ADAIR docker on the AI server. After installing these, one has to have folders as in apro_light_version_3 or the relevant folder from which the code is going to be called. One has to configure full paths with respect to the location of their folders as in this folder. Then one can run the jupyter notebook (*.ipynb). In the end, the .csv files will be generated and will be located in the output folder.

We made the code easy to use in the sense that the config parameters can also be updated in the jupyter notebook on the AI or AT&T server as True or False as follows:

```
config['feat_ext'].char = False
config['feat_ext'].word = True
config['feat_ext'].word3 = True
config['feat_ext'].pretrained_w2v = True
config['feat_ext'].transformer = True
```

## How to run it from within your local computer

The main code for ATexPro light on BitBucket (https://code.afiniti.com/projects/DATA/repos/nlp/browse?at=refs%2Fheads%2Fatex_pro_light) can also be pulled and run on one's local computer by creating a virtual environment on PyCharm. All the dependencies are stated in the requirements.txt file in the docs folder. However, as mentioned above, the creation of a virtual environment is NOT needed for the servers. 

## Credits

Code was written by Bilge Sipal Sert (bilge.sipal@afiniti.com), Cem Rifki Aydin (cem.aydin@afiniti.com), and Arjumand Younus.
